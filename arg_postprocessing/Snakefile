# This workflow is intended to be run with the --sdm=conda option,
# and most rules require either the sc2ts or pangolin environments.
# The base conda environment that snakemake is run from requires
# snakemake, tszip, pandas and tqdm in order to run the "run"
# rules (which cannot be used with "conda")
#
# It's not entirely automated in terms of parallelism. Running pangolin
# needs a lot of cores, so in practise you need to do something like this
# first to get the bit needing parallelism (and lots of memory) done
#
# snakemake sc2ts_viridian_v1.2.trees.tsz --cores=40
# 
# Then, the rest should just run like
#
# snakemake --cores=4
#

# Notes on saved files in the data directory
#
# recombinants.csv -> sc2ts_v1_2023-02-21_pr_pp_mp_aph_bps_pango_dated_recombinants_rebar_pangonet_nsl_akm.csv
# pango_x_events.csv -> sc2ts_v1_2023-02-21_pr_pp_mp_aph_bps_pango_dated_mmps_pango_x_events.csv
# deletion_events.csv -> sc2ts_v1_2023-02-21_pr_pp_mp_aph_bps_pango_dated_deletion_events.csv
rule all:
    input:
        # Base debug report
        "logs/sc2ts_v1_2023-02-21_debug-report.ipynb",
        # Rewiring recombinants report
        "logs/sc2ts_v1_2023-02-21_rewire-recombinants-report.ipynb",
        # Comparison between sc2ts and usher ARGs
        "logs/usher_sc2ts_comparison.ipynb",
        # Deletion events.
        "sc2ts_v1_2023-02-21_pr_pp_mp_aph_bps_pango_dated_deletion_events.csv",
        # Recombinant data. Suffixes after _recombinants denote the operations
        # that are applied to. This file is the source for the "recombinants.csv"
        # file in the top-level data directory.
        "sc2ts_v1_2023-02-21_pr_pp_mp_aph_bps_pango_dated_recombinants_rebar_pangonet_nsl_akm.csv",
        # Events corresponding to Pango X lineages. This is the pango_x_events.csv file
        # in the top-level data directory
        "sc2ts_v1_2023-02-21_pr_pp_mp_aph_bps_pango_dated_mmps_pango_x_events.csv",
        "logs/final_arg_report.ipynb",

rule final_args:
    conda: 
        "sc2ts.yml"
    input:
        # Final ARG
        # Order of operations:
        # 1) prune recombinants (_pr: rematching + long branch split)
        # 2) postprocess (_pp: various minor tweaks)
        # 3) Parsimony mapping (_mp: adding in deletions and recurrent sites)
        # 4) Apply node parsimony heuristics (_aph: cleanup some mess after 3)
        # 5) Shift breakpoints (must be after mapping: _bps)
        # 6) Compute pango assignment (must be after parsimony mapping and heuristics)
        # 7) Dating (_dated)
        # 8) Minimise metadata (must be last step)
        "sc2ts_v1_2023-02-21_pr_pp_mp_aph_bps_pango_dated_mmps.trees.tsz",
        "usher_v1_2024-06-06_mm.trees.tsz",
        "sc2ts_2023-02-21_intersection.trees.tsz",
        "usher_2023-02-21_intersection_ds_di.trees.tsz",
    output: 
        # Version numbers - mainly driven by the sc2ts_viridian_v1 version
        # as the others except usher_viridian are derived from this. These 
        # are major.minor, with major versions meaning significant changes,
        # minor being relatively minor tweaks to the ARG
        "sc2ts_viridian_v1.2.trees.tsz",
        "usher_viridian_v1.0.trees.tsz",
        "sc2ts_viridian_inter_v1.2.trees.tsz",
        "usher_viridian_inter_v1.2.trees.tsz",
    log:
        notebook="logs/final_arg_report.ipynb"
    notebook:
        "notebooks/final_arg_report.py.ipynb"
        

rule initialise:
    input:
        "../inference/results/v1-beta1/v1-beta1_{DATE}.ts",
    output:
        "sc2ts_v1_{DATE}.trees",
    shell:
        """
        cp {input} {output}
        """

rule debug_report:
    conda:
        "sc2ts.yml"
    input:
        "{DR_PREFIX}.trees",
    output:
        "{DR_PREFIX}_samples.csv",
        "{DR_PREFIX}_resources.csv",
    log:
        notebook="logs/{DR_PREFIX}_debug-report.ipynb"
    notebook:
        "notebooks/debug_report.py.ipynb"


rule rematch_recombinants_lbs:
    conda:
        "sc2ts.yml"
    input:
        "{RRL_PREFIX}.trees"
    output:
        "{RRL_PREFIX}_recombinants_lbs.json",
    shell:
        """
        python scripts/rematch_recombinants.py {input} {output}
        """

rule rewire_recombinants:
    conda:
        "sc2ts.yml"
    input:
        "{PR_PREFIX}.trees",
        "{PR_PREFIX}_recombinants_lbs.json",
    output:
        "{PR_PREFIX}_pr.trees"
    shell:
        """
        python -m sc2ts rewire-lbs {input} {output} -vv
        """

rule rewire_recombinants_report:
    conda:
        "sc2ts.yml"
    input:
        "{RRR_PREFIX}.trees",
        "{RRR_PREFIX}_recombinants_lbs.json",
        "{RRR_PREFIX}_pr.trees",
    log:
        notebook="logs/{RRR_PREFIX}_rewire-recombinants-report.ipynb"
    notebook:
        "notebooks/rewire-recombinants-report.py.ipynb"


rule rematch_recombinants:
    conda:
        "sc2ts.yml"
    input:
        "{RR_PREFIX}.trees"
    output:
        "{RR_PREFIX}_recombinants.json",
    shell:
        """
        python scripts/rematch_recombinants.py {input} \
            --pattern=../inference/results/v1-beta1/v1-beta1_{{date}}.ts \
            {output}
        """


# Add exact matches to the dataset and do some minor mutation
# rearrangements like pushing unary recombinant mutations.
rule postprocess:
    conda:
        "sc2ts.yml"
    input:
        "{PP_PREFIX}.trees"
    output:
        "{PP_PREFIX}_pp.trees",
    shell:
        """
        python -m sc2ts postprocess {input} {output} \
            --match-db=../inference/results/v1-beta1.matches.db
        """

rule map_parsimony:
    conda:
        "sc2ts.yml"
    input:
        "../data/viridian_mafft_2024-10-14_v1.vcz.zip",
        "{MP_PREFIX}.trees",
        "{MP_PREFIX}_sites_to_remap.txt",
    output:
        "{MP_PREFIX}_mp.trees",
        "{MP_PREFIX}_mp.csv",
    shell:
        """
        python -m sc2ts map-parsimony -vv \
            {input[0]} {input[1]} {output[0]} \
            --sites={input[2]} --report={output[1]} --no-progress
        """

rule apply_parsimony_heuristics:
    conda:
        "sc2ts.yml"
    input:
        "{APH_PREFIX}.trees"
    output:
        "{APH_PREFIX}_aph.trees",
        "{APH_PREFIX}_aph.csv",
    shell:
        """
        python -m sc2ts apply-node-parsimony {input} {output[0]} \
            --report={output[1]} --no-progress -vv
        """


rule deletion_report:
    conda:
        "sc2ts.yml"
    input:
        "{DELR_PREFIX}.trees",
    output:
        "{DELR_PREFIX}_deletion_events.csv",
    log:
        notebook="logs/{DELR_PREFIX}_deletion-report.ipynb"
    notebook:
        "notebooks/deletion_report.py.ipynb"


rule recombinants_report:
    conda:
        "sc2ts.yml"
    input:
        "{RECR_PREFIX}.trees",
    output:
        "{RECR_PREFIX}_recombinants.csv",
    log:
        notebook="logs/{RECR_PREFIX}_recombinants_report.ipynb"
    notebook:
        "notebooks/recombinants_report.py.ipynb"


rule pango_x_events:
    conda:
        "sc2ts.yml"
    input:
        "{PXE_PREFIX}.trees",
    output:
        "{PXE_PREFIX}_pango_x_events.csv",
    shell:
        """
        python scripts/compute_pango_x_events.py {input} {output}
        """

rule add_pangonet_to_csv:
    conda:
        "sc2ts.yml"
    input:
        "{APC_PREFIX}.csv",
    output:
        "{APC_PREFIX}_pangonet.csv",
    shell:
        """
        python scripts/add_pangonet_distance_to_csv.py {input} {output}
        """


rule add_net_supporting_loci_to_csv:
    conda:
        "sc2ts.yml"
    input:
        "{ASL_PREFIX}.trees",
        "{ASL_PREFIX}_recombinants_{ASL_SUFFIX}.csv",
    output:
        "{ASL_PREFIX}_recombinants_{ASL_SUFFIX}_nsl.csv",
    shell:
        """
        python scripts/add_recombinant_minlength_to_csv.py -v {input} {output}
        """

rule add_k1000_muts:
    conda:
        "sc2ts.yml"
    input:
        "{AKM_PREFIX}_recombinants.json",
        "{AKM_PREFIX}_recombinants_{AKM_SUFFIX}.csv",
    output:
        "{AKM_PREFIX}_recombinants_{AKM_SUFFIX}_akm.csv",
    shell:
        """
        python scripts/add_recombinant_k1000_muts_to_csv.py {input} {output}
        """

rule shift_breakpoints:
    conda:
        "sc2ts.yml"
    input:
        "{BP_PREFIX}.trees",
    output:
        "{BP_PREFIX}_bps.trees",
    log:
        "{BP_PREFIX}_shift_breakpoints.log",
    shell:
        """
        python scripts/run_breakpoint_shift_for_deletions.py -v {input} {output} > {log} 2>&1
        """


rule date:
    conda:
        "sc2ts.yml"
    input:
        "{DATE_PREFIX}.trees",
    output:
        "{DATE_PREFIX}_dated.trees",
    shell:
        """
        python scripts/run_nonsample_dating.py {input} {output}
        """


rule tszip:
    conda:
        "sc2ts.yml"
    input:
        "{TSZIP_PREFIX}.trees",
    output:
        "{TSZIP_PREFIX}.trees.tsz",
    shell:
        """
        python -m tszip -k {input} 
        """


rule write_fasta:
    conda:
        "sc2ts.yml"
    input:
        "{WF_PREFIX}.trees",
    output:
        "{WF_PREFIX}.fasta",

    shell:
        """
        python scripts/get_all_nodes_fasta.py {input} {output}
        """

rule run_pangolin:
    conda:
        "pangolin.yml"
    input:
        "{RP_PREFIX}.fasta",
    output:
        "{RP_PREFIX}.lineage_report.csv",
    shell:
        # Keep the temp files in a local directory pangolin_tmp because we create
        # several copies of the data during the process of running it.
        """
        mkdir -p pangolin_tmp
        pangolin -t {workflow.cores} {input} --tempdir=pangolin_tmp --outfile={output}
        """


rule add_pangolin_metadata:
    input:
        "{APM_PREFIX}.trees",
        "{APM_PREFIX}.lineage_report.csv",
    output:
        "{APM_PREFIX}_pango.trees",
    run:
        import tskit
        import pandas as pd
        import tqdm

        df = pd.read_csv(input[1]).set_index("taxon")
        # Remove NaNs from missing scorpio calls
        df["scorpio_call"] = df.scorpio_call.fillna(".")
        tables = tskit.TableCollection.load(input[0])
        nodes = tables.nodes.copy()
        tables.nodes.clear()

        for u, row in enumerate(tqdm.tqdm(nodes)):
            record = df.loc[f"n{u}"]
            row.metadata["pango"] = record["lineage"]
            row.metadata["scorpio"] = record["scorpio_call"]
            assert isinstance(record["scorpio_call"], str)
            tables.nodes.append(row)

        tables.dump(f"{output}")


rule minimise_metadata_pango_scorpio:
    conda:
        "sc2ts.yml"
    input:
        "{MM_PREFIX}.trees",
    output:
        "{MM_PREFIX}_mmps.trees",
    shell:
        # Convert the output of the add pangolin metadata command above
        # and drop the vestigial root edge
        """
        python -m sc2ts minimise-metadata {input} {output} \
            -m strain sample_id -m pango pango -m scorpio scorpio \
            --drop-vestigial-root
        """


rule minimise_metadata_date:
    conda:
        "sc2ts.yml"
    input:
        "{MM_PREFIX}.trees",
    output:
        "{MM_PREFIX}_mmd.trees",
    shell:
        # Add the Date_tree to the minimal metadata 
        """
        python -m sc2ts minimise-metadata {input} {output} \
            -m strain sample_id -m Date_tree Date_tree
        """


rule minimise_metadata:
    conda:
        "sc2ts.yml"
    input:
        "{MM_PREFIX}.trees",
    output:
        "{MM_PREFIX}_mm.trees",
    shell:
        # Minimal metadata, just keep sample_id
        """
        python -m sc2ts minimise-metadata {input} {output}
        """


########################
# Usher tree conversion
########################

rule download_json:
    output:
        "tree.all_viridian.202409.jsonl.gz"
    shell:
        """
        wget --quiet --content-disposition \
            https://figshare.com/ndownloader/files/49691040 \
            -O {output}
        """

rule download_pb:
    output:
        "tree.all_viridian.202409.pb.gz"
    shell:
        """
        wget --quiet --content-disposition \
            https://figshare.com/ndownloader/files/49691037 \
            -O {output}
        """

rule export_mutations_local:
    conda:
        # The pangolin environment above also contains a full copy
        # of Usher/matutils, so no point in redoing that.
        "pangolin.yml"
    input:
        "tree.all_viridian.202409.pb.gz"
    output:
        "usher_mutations.tsv"
    shell:
        # Got GTF from 
        # http://hgdownload.soe.ucsc.edu/goldenPath/wuhCor1/bigZips/genes/ncbiGenes.gtf.gz
        """
        matUtils summary -i tree.all_viridian.202409.pb.gz \
            -f reference.fasta -g ncbiGenes.gtf \
            --translate usher_mutations.tsv
        """

rule convert_topology:
    conda:
        "sc2ts.yml"
    input:
        "tree.all_viridian.202409.jsonl.gz"
    output:
        "usher_topology.trees"
    shell:
        "python scripts/mat2tsk.py convert-topology {input} {output}"

rule add_mutations:
    conda:
        "sc2ts.yml"
    input:
        "usher_topology.trees",
        "usher_mutations.tsv"
    output:
        "usher_v1_2024-06-06.trees"
    shell:
        """
        python scripts/mat2tsk.py convert-mutations {input} reference.fasta {output}
        """

rule date_samples:
    conda:
        "sc2ts.yml"
    input:
        # Note: call with e.g. snakemake usher_v1_2024-06-06_ds.trees
        "{DS_PREFIX}.trees",
    output:
        "{DS_PREFIX}_ds.trees",
    shell:
        """
        python scripts/mat2tsk.py date-samples {input} {output}
        """

rule date_internal_nodes:
    conda:
        "sc2ts.yml"
    input:
        # Note: call with e.g. snakemake usher_v1_2024-06-06_ds_di.trees
        "{DI_PREFIX}.trees"
    output:
        "{DI_PREFIX}_di.trees"
    shell:
        """
        echo "NOTE: tsdate can take many minutes per EP iteration: this may be a while."
        python scripts/mat2tsk.py date-internal {input} {output}
        """

rule sc2ts_usher_intersection:
    conda:
        "sc2ts.yml"
    input:
        "usher_v1_2024-06-06_mmd.trees",
        # We use the final trees after post-processing and applying parsimony mapping
        "sc2ts_v1_2023-02-21_pr_pp_mp_aph_mm.trees"
    output:
        "usher_2023-02-21_intersection.trees",
        "sc2ts_2023-02-21_intersection.trees",
    shell:
        """
        python scripts/mat2tsk.py intersect --intersect-sites {input} {output}
        """


rule generate_all_sites:
    output: "all_sites.txt"
    shell: "seq 1 29903 > {output}"


rule all_sites_parsimony:
    conda:
        "sc2ts.yml"
    input:
        "../data/viridian_mafft_2024-10-14_v1.vcz.zip",
        "{ASP_PREFIX}.trees",
        "all_sites.txt",
    output:
        "{ASP_PREFIX}_asp.trees",
        "{ASP_PREFIX}_asp.csv"
    shell:
        """
        python -m sc2ts map-parsimony -v {input[0]} {input[1]} {output[0]} \
            --report={output[1]} --no-progress \
            --sites={input[2]}
        """


rule usher_sc2ts_comparison:
    conda:
        "sc2ts.yml"
    input:
        "usher_2023-02-21_intersection.trees",
        "usher_2023-02-21_intersection_asp.trees",
        "usher_2023-02-21_intersection_asp.csv",
        "sc2ts_2023-02-21_intersection.trees",
        "sc2ts_2023-02-21_intersection_asp.trees",
        "sc2ts_2023-02-21_intersection_asp.csv",
    log:
        notebook="logs/usher_sc2ts_comparison.ipynb"

    notebook:
        "notebooks/usher_sc2ts_comparison.py.ipynb"


################
# Rebar stuff
################

rule rebar_dataset:
    conda: "rebar.yml"
    output: "rebar/summary.json"
    shell:
      """
      rebar dataset download \
        --name sars-cov-2 \
        --tag 2025-01-28 \
        --verbosity error \
        --output-dir rebar
      """


rule run_rebar:
    conda: "rebar.yml"
    input: 
        "rebar/summary.json",
        "{RR_PREFIX}_recombinants.fasta"
    output:
        "{RR_PREFIX}_rebar_output.tsv"
    shell:
      """
      rebar run \
          --dataset-dir rebar \
          --threads {workflow.cores} \
          --alignment {input[1]} \
          --output-dir rebar_output
      cp rebar_output/linelist.tsv {output}
      """
        
rule extract_recombinant_alignments:
    conda: "sc2ts.yml"
    input:
        "../data/viridian_mafft_2024-10-14_v1.vcz.zip",
        "{ERA_PREFIX}_recombinants.csv",
    output:
        "{ERA_PREFIX}_recombinants.fasta",
    shell:
        """
        python scripts/get_sample_fasta.py {input} {output}
        """


rule add_rebar_to_csv:
    conda: "sc2ts.yml"
    input: 
        "{ARC_PREFIX}_recombinants.csv",
        "{ARC_PREFIX}_rebar_output.tsv",
    output:
        "{ARC_PREFIX}_recombinants_rebar.csv",
    shell:
        """
        python scripts/add_rebar_to_csv.py {input} {output}
        """


#######
# Sites for remapping
#######

rule lineage_mutations:
    output:
        "lineage_mutations.json"
    shell:
        """
        curl -o {output} https://raw.githubusercontent.com/andersen-lab/Freyja/refs/heads/main/freyja/data/lineage_mutations.json
        """


rule deletion_site_data:
    output:
        "Li_et_al_TableS1.xlsx"
    shell:
        """
        curl -o {output} https://figshare.com/ndownloader/files/40119535
        """

rule generate_sites_for_remapping:
    conda: "sc2ts.yml"
    input: 
        "{GSR_PREFIX}.trees",
        "Li_et_al_TableS1.xlsx",
        "lineage_mutations.json",
    output:
        # We store a CSV and text file so that we can see the reason
        # for analysis and also just pass the list to sc2ts.
        "{GSR_PREFIX}_sites_to_remap.csv",
        "{GSR_PREFIX}_sites_to_remap.txt",
    shell:
        """
        python scripts/generate_sites_for_remapping.py {input} {output}
        """
